{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources \n",
    "https://www.kaggle.com/startupsci/titanic-data-science-solutions   Basic feature engineering basic modeling. Initial fork.  \n",
    "https://www.kaggle.com/ldfreeman3/a-data-science-framework-to-achieve-99-accuracy  A look into hyperparameterization with sklearn  \n",
    "https://www.kaggle.com/shunjiangxu/blood-is-thicker-than-water-friendship-forever/notebook  Advanced feature engineering covering group/family survival assumption feature\n",
    "\n",
    "\n",
    "https://www.analyticsvidhya.com/blog/2017/09/understaing-support-vector-machine-example-code/ Support Vector Machines  \n",
    "https://medium.com/deep-math-machine-learning-ai/chapter-3-support-vector-machine-with-math-47d6193c82be Support Vector Machines with math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import random as rnd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(r\"C:\\Users\\heret\\Downloads\\titanic\\train.csv\")\n",
    "test = pd.read_csv(r\"C:\\Users\\heret\\Downloads\\titanic\\test.csv\")\n",
    "\n",
    "all_data = pd.concat((train, test), sort=False).reset_index(drop=True)\n",
    "ntrain = train.shape[0]\n",
    "ntest = test.shape[0]\n",
    "all_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discretise Sex variable\n",
    "all_data['Sex'] = all_data['Sex'].map( {'female': 1, 'male': 0} ).astype(int)\n",
    "\n",
    "# Replace Age NaNs by imputing them based on Sex and Pclass median\n",
    "guess_ages = np.zeros((2,3))\n",
    "for i in range(0, 2):\n",
    "    for j in range(0, 3):\n",
    "        guess_df = all_data[(all_data['Sex'] == i) & (all_data['Pclass'] == j+1)]['Age'].dropna()\n",
    "        age_guess = guess_df.median()\n",
    "\n",
    "        # Convert random age float to nearest .5 age\n",
    "        guess_ages[i,j] = int( age_guess/0.5 + 0.5 ) * 0.5\n",
    "\n",
    "for i in range(0, 2):\n",
    "    for j in range(0, 3):\n",
    "        all_data.loc[ (all_data.Age.isnull()) & (all_data.Sex == i) & (all_data.Pclass == j+1), 'Age'] = guess_ages[i,j]\n",
    "\n",
    "all_data['Age'] = all_data['Age'].astype(int)\n",
    "\n",
    "\n",
    "# Replacing Embarked NaNs with the mode\n",
    "freq_port = all_data.Embarked.mode()[0]\n",
    "all_data['Embarked'] = all_data['Embarked'].fillna(freq_port)\n",
    "all_data['Embarked'] = all_data['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int) #turning into a numeric variable\n",
    "\n",
    "# Replacing Fare NaN with median\n",
    "all_data['Fare'].fillna(all_data['Fare'].dropna().median(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a new IsAlone varible\n",
    "all_data[\"FamilySize\"] = all_data[\"SibSp\"] + all_data[\"Parch\"] + 1\n",
    "all_data[\"IsAlone\"] = 0\n",
    "all_data.loc[all_data[\"FamilySize\"] == 1, \"IsAlone\"] = 1\n",
    "#print(all_data[[\"IsAlone\",\"Survived\"]].groupby(by=\"IsAlone\", as_index=False).mean()) #there seems to be correlation within these groups\n",
    "\n",
    "# Creating a new variable \"Last name\" so it can help us identify families\n",
    "all_data['Last name'] = all_data['Name'].apply(lambda x: str.split(x, \",\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a new variable Family survival with info whether the family survived or not, this is inferred by grouping last name and fare duplicated values\n",
    "# all credit to https://www.kaggle.com/shunjiangxu/blood-is-thicker-than-water-friendship-forever/notebook\n",
    "\n",
    "grp_partial_age = 0\n",
    "grp_partial_cabin = 0\n",
    "grp_age_diff_df = pd.DataFrame()\n",
    "all_data['Family survival'] = 0.5\n",
    "\n",
    "for grp, grp_df in all_data[['Survived','Name', 'Last name', 'Fare', 'SibSp', 'Parch', 'Age', 'Cabin']].groupby(['Last name', 'Fare']):\n",
    "    if (len(grp_df) != 1):\n",
    "        grp_missing_age = len(grp_df[grp_df['Age'].isnull()])\n",
    "        is_partial_age = (grp_missing_age != 0) & (grp_missing_age != len(grp_df))\n",
    "        grp_partial_age += is_partial_age\n",
    "        \n",
    "        sibsp_df = grp_df.loc[grp_df['SibSp']!=0, ['Age']]\n",
    "        #print(sibsp_df.info())\n",
    "        sibsp_age_diff = sibsp_df.max() - sibsp_df.min()\n",
    "        grp_age_diff_df = grp_age_diff_df.append(sibsp_age_diff, ignore_index=True)\n",
    "        \n",
    "        grp_missing_cabin = len(grp_df[grp_df['Cabin'].isnull()])\n",
    "        grp_partial_cabin += (grp_missing_cabin != 0) & (grp_missing_cabin != len(grp_df))\n",
    "\n",
    "        for PassID, row in grp_df.iterrows():\n",
    "            ## Find out if any family memebers survived or not\n",
    "            smax = grp_df.drop(PassID)['Survived'].max()\n",
    "            smin = grp_df.drop(PassID)['Survived'].min()\n",
    "\n",
    "            ## If any family memebers survived, put this feature as 1\n",
    "            if (smax==1.0): all_data.loc[PassID, 'Family survival'] = 1\n",
    "            ## Otherwise if any family memebers perished, put this feature as 0\n",
    "            elif (smin==0.0): all_data.loc[PassID, 'Family survival'] = 0\n",
    "\n",
    "\n",
    "# Some ticket numbers and fares are the same suggesting they may be groups and not families, which leads to the assumption that they may have survived or died together.\n",
    "# We will overload the 'Family survival' column instead of creating a seperate feature.\n",
    "grp_partial_age = 0\n",
    "grp_partial_cabin = 0\n",
    "grp_age_diff_df = pd.DataFrame(columns=['Age diff'])\n",
    "ticket_grpby = all_data.groupby('Ticket')\n",
    "for _, grp_df in ticket_grpby:\n",
    "    if (len(grp_df) > 1):\n",
    "        grp_missing_age = len(grp_df[grp_df['Age'].isnull()])\n",
    "        grp_partial_age += (grp_missing_age != 0) & (grp_missing_age != len(grp_df))\n",
    "\n",
    "        grp_age_diff_df = grp_age_diff_df.append(pd.DataFrame(data=[grp_df['Age'].max() - grp_df['Age'].min()], columns=['Age diff']))\n",
    "\n",
    "        grp_missing_cabin = len(grp_df[grp_df['Cabin'].isnull()])\n",
    "        grp_partial_cabin += (grp_missing_cabin != 0) & (grp_missing_cabin != len(grp_df))\n",
    "        for PassID, row in grp_df.iterrows():\n",
    "            if (row['Family survival']==0)|(row['Family survival']==0.5):\n",
    "                smax = grp_df.drop(PassID)['Survived'].max()\n",
    "                smin = grp_df.drop(PassID)['Survived'].min()\n",
    "                if (smax==1.0): \n",
    "                    all_data.loc[PassID, 'Family survival'] = 1\n",
    "                elif (smin==0.0): \n",
    "                    all_data.loc[PassID, 'Family survival'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a new Title variable\n",
    "all_data[\"Title\"] = all_data.Name.str.extract(\" ([A-Za-z]+)\\.\", expand=False)\n",
    "print(all_data[[\"Title\", \"Survived\"]].groupby([\"Title\"], as_index=False).count())\n",
    "\n",
    "# Now grouping all minor categories into one\n",
    "all_data[\"Title\"] = all_data[\"Title\"].replace([\"Lady\", \"Countess\",\"Capt\", \"Col\", \"Don\", \"Dr\", \"Major\", \"Rev\", \"Sir\", \"Jonkheer\", \"Dona\"], \"Rare\")\n",
    "all_data[\"Title\"] = all_data[\"Title\"].replace(\"Mlle\", \"Miss\")\n",
    "all_data[\"Title\"] = all_data[\"Title\"].replace(\"Ms\", \"Miss\")\n",
    "all_data[\"Title\"] = all_data[\"Title\"].replace(\"Mme\", \"Mrs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turning into a numeric variable\n",
    "title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\n",
    "all_data['Title'] = all_data['Title'].map(title_mapping)\n",
    "all_data['Title'] = all_data['Title'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binning Fare variable\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "all_data['FareBin_4'] = pd.qcut(all_data[\"Fare\"], 5) #group into 5 cuts based on 5 quartiles\n",
    "all_data['FareBin_4'] = LabelEncoder().fit_transform(all_data[\"FareBin_4\"]) #encodes it into a numerical variable\n",
    "\n",
    "# Binning Age variable\n",
    "all_data['Age_5'] = pd.qcut(all_data[\"Age\"], 5) #group into 5 cuts based on 5 quartiles\n",
    "all_data['Age_5'] = LabelEncoder().fit_transform(all_data[\"Age_5\"]) #encodes it into a numerical variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Dropping unnecessary columns\n",
    "all_data = all_data.drop(columns=[\"Name\", \"PassengerId\", \"Ticket\", \"Cabin\",\n",
    "                                  \"Fare\", \"Age\", \"SibSp\", \"Parch\", \"Last name\", \"FamilySize\"],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = train[\"Survived\"].values\n",
    "train_X = all_data.drop(columns=[\"Survived\"],axis=1)[:ntrain].values\n",
    "test_X = all_data.drop(columns=[\"Survived\"],axis=1)[ntrain:].values\n",
    "print(\"train_y: \", train_y.shape,\"    train_X: \", train_X.shape,\"   test_X: \", test_X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression, Perceptron, SGDClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import cross_val_score, KFold, ShuffleSplit, StratifiedShuffleSplit, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.feature_selection import RFECV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [LGBMClassifier(), LogisticRegression(), SVC(), LinearSVC(),\n",
    "          RandomForestClassifier(), KNeighborsClassifier(),GaussianNB(),\n",
    "          Perceptron(), SGDClassifier(), DecisionTreeClassifier()]\n",
    "\n",
    "for model in models:\n",
    "    classifier = model\n",
    "    print(model.__class__.__name__, cross_val_score(classifier, train_X, train_y, cv=5, scoring=\"accuracy\").mean().round(4))\n",
    "    \n",
    "#Logistic Regression CV 0.8406\n",
    "#Support Vector Machines CV 0.844 / LB 0.80861 \n",
    "#LightGBM CV 0.855 / LB 0.77990"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv_split = ShuffleSplit(n_splits = 10, test_size = .3, train_size = .6, random_state = 0)\n",
    "# param_grid = {\n",
    "#     'bootstrap': [True],\n",
    "#     'max_depth': [80, 90, 100, 110],\n",
    "#     'max_features': [2, 3],\n",
    "#     'min_samples_leaf': [3, 4, 5],\n",
    "#     'min_samples_split': [8, 10, 12],\n",
    "#     'n_estimators': [100, 200, 300, 1000]\n",
    "# } # \n",
    "\n",
    "# forest_rfe = RFECV(RandomForestClassifier(random_state = 0), step = 1, scoring = 'accuracy', cv = cv_split)\n",
    "# forest_rfe.fit(train_X, train_y)\n",
    "\n",
    "# grid_search = GridSearchCV(RandomForestClassifier(random_state = 0), param_grid=param_grid, scoring = 'accuracy', cv = cv_split, refit=True) #scoring = roc_auc\n",
    "# grid_search.fit(train_X[:,forest_rfe.get_support()], train_y) # we split the train_X set with the variables obtained through rfe\n",
    "# print(grid_search.best_params_)\n",
    "# print(grid_search.best_score_.round(3)) \n",
    "# {'bootstrap': True, 'max_depth': 80, 'max_features': 2, 'min_samples_leaf': 3, 'min_samples_split': 8, 'n_estimators': 100}\n",
    "# accuracy = base model 0.8272  vs  tuned model 0.859  vs  Public LB 0.78468\n",
    "\n",
    "# model = RandomForestClassifier(bootstrap=True, max_depth=80, max_features=2, min_samples_leaf=3, min_samples_split=8, n_estimators=100)\n",
    "# model.fit(train_X[:,forest_rfe.get_support()], train_y)\n",
    "# predictions = model.predict(test_X[:,forest_rfe.get_support()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv_split = ShuffleSplit(n_splits = 10, test_size = .3, train_size = .6, random_state = 0)\n",
    "# param_grid = {\"kernel\": [\"rbf\"],\n",
    "#               \"C\": [0.001, 0.01, 0.1, 1, 10],\n",
    "#               \"gamma\": [0.001, 0.01, 0.1, 1]} #accuracy = 0.847 vs base model 0.843\n",
    "\n",
    "# #svc_ref = RFECV(SVC(random_state = 0, kernel=\"rbf\"), step = 1, scoring = 'accuracy', cv = cv_split) if kernel=\"lineal\" all features are deemed meaningful to this model\n",
    "# #svc_ref.fit(train_X, train_y)\n",
    "# #svc_ref.get_support()\n",
    "\n",
    "# grid_search = GridSearchCV(SVC(random_state = 0), param_grid=param_grid, scoring = 'accuracy', cv = cv_split, refit=True) #scoring = roc_auc\n",
    "# grid_search.fit(train_X, train_y)\n",
    "# print(grid_search.best_params_)\n",
    "# print(grid_search.best_score_.round(3)) #accuracy = 0.847, LB = 0.76555"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local CV improves with GridSearch params, however default params work best in LB\n",
    "# LB score 0.80861\n",
    "model = SVC()\n",
    "model.fit(train_X, train_y)\n",
    "predictions = model.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_submission = pd.DataFrame({\"PassengerID\": test[\"PassengerId\"], \"Survived\": predictions})\n",
    "my_submission.to_csv(\"titanic.csv\", index=False)\n",
    "print(my_submission.head(10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
