{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/pmarcelino/comprehensive-data-exploration-with-python  \n",
    "https://www.kaggle.com/apapiu/regularized-linear-models  \n",
    "https://www.kaggle.com/juliencs/a-study-on-regression-applied-to-the-ames-dataset  \n",
    "https://www.kaggle.com/humananalog/xgboost-lasso  \n",
    "https://www.kaggle.com/serigne/stacked-regressions-top-4-on-leaderboard\n",
    "\n",
    "http://blog.kaggle.com/2017/06/15/stacking-made-easy-an-introduction-to-stacknet-by-competitions-grandmaster-marios-michailidis-kazanova/ Intro to stacking  \n",
    "https://dnc1994.com/2016/05/rank-10-percent-in-first-kaggle-competition-en/ Basic tips to improve ranking  \n",
    "https://mlwave.com/kaggle-ensembling-guide/  Ensembling guide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read the data\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "from scipy.stats import norm\n",
    "from scipy import stats\n",
    "from scipy.stats import skew\n",
    "from scipy.stats.stats import pearsonr\n",
    "\n",
    "train_X = pd.read_csv('../input/train.csv')\n",
    "test_X = pd.read_csv('../input/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the 'Id' column\n",
    "train_ID = train_X['Id']\n",
    "test_ID = test_X['Id']\n",
    "\n",
    "#Now drop the  'Id' colum since it's unnecessary for  the prediction process.\n",
    "train_X.drop(\"Id\", axis = 1, inplace = True)\n",
    "test_X.drop(\"Id\", axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(\"seaborn-darkgrid\")\n",
    "plt.scatter(train_X.GrLivArea, train_X.SalePrice)\n",
    "plt.title(\"Looking for outliers\")\n",
    "plt.xlabel(\"GrLivArea\")\n",
    "plt.ylabel(\"SalePrice\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = train_X[train_X.GrLivArea < 4000] #the author of the dataset recommends removing 'any houses with more than 4000 square feet' from the dataset\n",
    "\n",
    "# The test example 1116 only has GarageType but no other information. We'll assume it does not have a garage.\n",
    "test_X.loc[1116, \"GarageType\"] = np.nan\n",
    "# The test example with ID 666 has GarageArea, GarageCars, and GarageType but none of the other fields, so use the mode and median to fill them in.\n",
    "test_X.loc[666, \"GarageQual\"] = \"TA\"\n",
    "test_X.loc[666, \"GarageCond\"] = \"TA\"\n",
    "test_X.loc[666, \"GarageFinish\"] = \"Unf\"\n",
    "test_X.loc[666, \"GarageYrBlt\"] = \"1980\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X.SalePrice = np.log1p(train_X.SalePrice)\n",
    "train_y = train_X.SalePrice.values #creates a log transformed y variable\n",
    "\n",
    "ntrain = train_X.shape[0]\n",
    "ntest = test_X.shape[0]\n",
    "all_data = pd.concat((train_X, test_X), sort=False).reset_index(drop=True)\n",
    "all_data.drop(['SalePrice'], axis=1, inplace=True)\n",
    "print(\"all_data size is : {}\".format(all_data.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values for features where median/mean or most common value doesn't make sense\n",
    "\n",
    "# Alley : data description says NA means \"no alley access\"\n",
    "all_data.loc[:, \"Alley\"] = all_data.loc[:, \"Alley\"].fillna(\"None\")\n",
    "# BedroomAbvGr : NA most likely means 0\n",
    "all_data.loc[:, \"BedroomAbvGr\"] = all_data.loc[:, \"BedroomAbvGr\"].fillna(0)\n",
    "# BsmtQual etc : data description says NA for basement features is \"no basement\"\n",
    "all_data.loc[:, \"BsmtQual\"] = all_data.loc[:, \"BsmtQual\"].fillna(\"No\")\n",
    "all_data.loc[:, \"BsmtCond\"] = all_data.loc[:, \"BsmtCond\"].fillna(\"No\")\n",
    "all_data.loc[:, \"BsmtExposure\"] = all_data.loc[:, \"BsmtExposure\"].fillna(\"No\")\n",
    "all_data.loc[:, \"BsmtFinType1\"] = all_data.loc[:, \"BsmtFinType1\"].fillna(\"No\")\n",
    "all_data.loc[:, \"BsmtFinType2\"] = all_data.loc[:, \"BsmtFinType2\"].fillna(\"No\")\n",
    "all_data.loc[:, \"BsmtFullBath\"] = all_data.loc[:, \"BsmtFullBath\"].fillna(0)\n",
    "all_data.loc[:, \"BsmtHalfBath\"] = all_data.loc[:, \"BsmtHalfBath\"].fillna(0)\n",
    "all_data.loc[:, \"BsmtUnfSF\"] = all_data.loc[:, \"BsmtUnfSF\"].fillna(0)\n",
    "# CentralAir : NA most likely means No\n",
    "all_data.loc[:, \"CentralAir\"] = all_data.loc[:, \"CentralAir\"].fillna(\"N\")\n",
    "# Condition : NA most likely means Normal\n",
    "all_data.loc[:, \"Condition1\"] = all_data.loc[:, \"Condition1\"].fillna(\"Norm\")\n",
    "all_data.loc[:, \"Condition2\"] = all_data.loc[:, \"Condition2\"].fillna(\"Norm\")\n",
    "# EnclosedPorch : NA most likely means no enclosed porch\n",
    "all_data.loc[:, \"EnclosedPorch\"] = all_data.loc[:, \"EnclosedPorch\"].fillna(0)\n",
    "# External stuff : NA most likely means average\n",
    "all_data.loc[:, \"ExterCond\"] = all_data.loc[:, \"ExterCond\"].fillna(\"TA\")\n",
    "all_data.loc[:, \"ExterQual\"] = all_data.loc[:, \"ExterQual\"].fillna(\"TA\")\n",
    "# Fence : data description says NA means \"no fence\"\n",
    "all_data.loc[:, \"Fence\"] = all_data.loc[:, \"Fence\"].fillna(\"No\")\n",
    "# FireplaceQu : data description says NA means \"no fireplace\"\n",
    "all_data.loc[:, \"FireplaceQu\"] = all_data.loc[:, \"FireplaceQu\"].fillna(\"No\")\n",
    "all_data.loc[:, \"Fireplaces\"] = all_data.loc[:, \"Fireplaces\"].fillna(0)\n",
    "# Functional : data description says NA means typical\n",
    "all_data.loc[:, \"Functional\"] = all_data.loc[:, \"Functional\"].fillna(\"Typ\")\n",
    "# GarageType etc : data description says NA for garage features is \"no garage\"\n",
    "all_data.loc[:, \"GarageType\"] = all_data.loc[:, \"GarageType\"].fillna(\"No\")\n",
    "all_data.loc[:, \"GarageFinish\"] = all_data.loc[:, \"GarageFinish\"].fillna(\"No\")\n",
    "all_data.loc[:, \"GarageQual\"] = all_data.loc[:, \"GarageQual\"].fillna(\"No\")\n",
    "all_data.loc[:, \"GarageCond\"] = all_data.loc[:, \"GarageCond\"].fillna(\"No\")\n",
    "all_data.loc[:, \"GarageArea\"] = all_data.loc[:, \"GarageArea\"].fillna(0)\n",
    "all_data.loc[:, \"GarageCars\"] = all_data.loc[:, \"GarageCars\"].fillna(0)\n",
    "# HalfBath : NA most likely means no half baths above grade\n",
    "all_data.loc[:, \"HalfBath\"] = all_data.loc[:, \"HalfBath\"].fillna(0)\n",
    "# HeatingQC : NA most likely means typical\n",
    "all_data.loc[:, \"HeatingQC\"] = all_data.loc[:, \"HeatingQC\"].fillna(\"TA\")\n",
    "# KitchenAbvGr : NA most likely means 0\n",
    "all_data.loc[:, \"KitchenAbvGr\"] = all_data.loc[:, \"KitchenAbvGr\"].fillna(0)\n",
    "# KitchenQual : NA most likely means typical\n",
    "all_data.loc[:, \"KitchenQual\"] = all_data.loc[:, \"KitchenQual\"].fillna(\"TA\")\n",
    "# LotFrontage : NA most likely means no lot frontage\n",
    "all_data.loc[:, \"LotFrontage\"] = all_data.loc[:, \"LotFrontage\"].fillna(0)\n",
    "# LotShape : NA most likely means regular\n",
    "all_data.loc[:, \"LotShape\"] = all_data.loc[:, \"LotShape\"].fillna(\"Reg\")\n",
    "# MasVnrType : NA most likely means no veneer\n",
    "all_data.loc[:, \"MasVnrType\"] = all_data.loc[:, \"MasVnrType\"].fillna(\"None\")\n",
    "all_data.loc[:, \"MasVnrArea\"] = all_data.loc[:, \"MasVnrArea\"].fillna(0)\n",
    "# MiscFeature : data description says NA means \"no misc feature\"\n",
    "all_data.loc[:, \"MiscFeature\"] = all_data.loc[:, \"MiscFeature\"].fillna(\"No\")\n",
    "all_data.loc[:, \"MiscVal\"] = all_data.loc[:, \"MiscVal\"].fillna(0)\n",
    "# OpenPorchSF : NA most likely means no open porch\n",
    "all_data.loc[:, \"OpenPorchSF\"] = all_data.loc[:, \"OpenPorchSF\"].fillna(0)\n",
    "# PavedDrive : NA most likely means not paved\n",
    "all_data.loc[:, \"PavedDrive\"] = all_data.loc[:, \"PavedDrive\"].fillna(\"N\")\n",
    "# PoolQC : data description says NA means \"no pool\"\n",
    "all_data.loc[:, \"PoolQC\"] = all_data.loc[:, \"PoolQC\"].fillna(\"No\")\n",
    "all_data.loc[:, \"PoolArea\"] = all_data.loc[:, \"PoolArea\"].fillna(0)\n",
    "# SaleCondition : NA most likely means normal sale\n",
    "all_data.loc[:, \"SaleCondition\"] = all_data.loc[:, \"SaleCondition\"].fillna(\"Normal\")\n",
    "# ScreenPorch : NA most likely means no screen porch\n",
    "all_data.loc[:, \"ScreenPorch\"] = all_data.loc[:, \"ScreenPorch\"].fillna(0)\n",
    "# TotRmsAbvGrd : NA most likely means 0\n",
    "all_data.loc[:, \"TotRmsAbvGrd\"] = all_data.loc[:, \"TotRmsAbvGrd\"].fillna(0)\n",
    "# Utilities : NA most likely means all public utilities\n",
    "all_data.loc[:, \"Utilities\"] = all_data.loc[:, \"Utilities\"].fillna(\"AllPub\")\n",
    "# WoodDeckSF : NA most likely means no wood deck\n",
    "all_data.loc[:, \"WoodDeckSF\"] = all_data.loc[:, \"WoodDeckSF\"].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some numerical features are actually really categories\n",
    "all_data = all_data.replace({\"MSSubClass\" : {20 : \"SC20\", 30 : \"SC30\", 40 : \"SC40\", 45 : \"SC45\", \n",
    "                                       50 : \"SC50\", 60 : \"SC60\", 70 : \"SC70\", 75 : \"SC75\", \n",
    "                                       80 : \"SC80\", 85 : \"SC85\", 90 : \"SC90\", 120 : \"SC120\", \n",
    "                                       150 : \"SC150\", 160 : \"SC160\", 180 : \"SC180\", 190 : \"SC190\"},\n",
    "                       \"MoSold\" : {1 : \"Jan\", 2 : \"Feb\", 3 : \"Mar\", 4 : \"Apr\", 5 : \"May\", 6 : \"Jun\",\n",
    "                                   7 : \"Jul\", 8 : \"Aug\", 9 : \"Sep\", 10 : \"Oct\", 11 : \"Nov\", 12 : \"Dec\"}\n",
    "                      })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode some categorical features as ordered numbers when there is information in the order\n",
    "all_data = all_data.replace({\"Alley\" : {\"Grvl\" : 1, \"Pave\" : 2},\n",
    "                       \"BsmtCond\" : {\"No\" : 0, \"Po\" : 1, \"Fa\" : 2, \"TA\" : 3, \"Gd\" : 4, \"Ex\" : 5},\n",
    "                       \"BsmtExposure\" : {\"No\" : 0, \"Mn\" : 1, \"Av\": 2, \"Gd\" : 3},\n",
    "                       \"BsmtFinType1\" : {\"No\" : 0, \"Unf\" : 1, \"LwQ\": 2, \"Rec\" : 3, \"BLQ\" : 4, \n",
    "                                         \"ALQ\" : 5, \"GLQ\" : 6},\n",
    "                       \"BsmtFinType2\" : {\"No\" : 0, \"Unf\" : 1, \"LwQ\": 2, \"Rec\" : 3, \"BLQ\" : 4, \n",
    "                                         \"ALQ\" : 5, \"GLQ\" : 6},\n",
    "                       \"BsmtQual\" : {\"No\" : 0, \"Po\" : 1, \"Fa\" : 2, \"TA\": 3, \"Gd\" : 4, \"Ex\" : 5},\n",
    "                       \"ExterCond\" : {\"Po\" : 1, \"Fa\" : 2, \"TA\": 3, \"Gd\": 4, \"Ex\" : 5},\n",
    "                       \"ExterQual\" : {\"Po\" : 1, \"Fa\" : 2, \"TA\": 3, \"Gd\": 4, \"Ex\" : 5},\n",
    "                       \"FireplaceQu\" : {\"No\" : 0, \"Po\" : 1, \"Fa\" : 2, \"TA\" : 3, \"Gd\" : 4, \"Ex\" : 5},\n",
    "                       \"Functional\" : {\"Sal\" : 1, \"Sev\" : 2, \"Maj2\" : 3, \"Maj1\" : 4, \"Mod\": 5, \n",
    "                                       \"Min2\" : 6, \"Min1\" : 7, \"Typ\" : 8},\n",
    "                       \"GarageCond\" : {\"No\" : 0, \"Po\" : 1, \"Fa\" : 2, \"TA\" : 3, \"Gd\" : 4, \"Ex\" : 5},\n",
    "                       \"GarageQual\" : {\"No\" : 0, \"Po\" : 1, \"Fa\" : 2, \"TA\" : 3, \"Gd\" : 4, \"Ex\" : 5},\n",
    "                       \"HeatingQC\" : {\"Po\" : 1, \"Fa\" : 2, \"TA\" : 3, \"Gd\" : 4, \"Ex\" : 5},\n",
    "                       \"KitchenQual\" : {\"Po\" : 1, \"Fa\" : 2, \"TA\" : 3, \"Gd\" : 4, \"Ex\" : 5},\n",
    "                       \"LandSlope\" : {\"Sev\" : 1, \"Mod\" : 2, \"Gtl\" : 3},\n",
    "                       \"LotShape\" : {\"IR3\" : 1, \"IR2\" : 2, \"IR1\" : 3, \"Reg\" : 4},\n",
    "                       \"PavedDrive\" : {\"N\" : 0, \"P\" : 1, \"Y\" : 2},\n",
    "                       \"PoolQC\" : {\"No\" : 0, \"Fa\" : 1, \"TA\" : 2, \"Gd\" : 3, \"Ex\" : 4},\n",
    "                       \"Street\" : {\"Grvl\" : 1, \"Pave\" : 2},\n",
    "                       \"Utilities\" : {\"ELO\" : 1, \"NoSeWa\" : 2, \"NoSewr\" : 3, \"AllPub\" : 4}}\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new features\n",
    "# 1* Simplifications of existing features\n",
    "all_data[\"SimplOverallQual\"] = all_data.OverallQual.replace({1 : 1, 2 : 1, 3 : 1, # bad\n",
    "                                                       4 : 2, 5 : 2, 6 : 2, # average\n",
    "                                                       7 : 3, 8 : 3, 9 : 3, 10 : 3 # good\n",
    "                                                      })\n",
    "all_data[\"SimplOverallCond\"] = all_data.OverallCond.replace({1 : 1, 2 : 1, 3 : 1, # bad\n",
    "                                                       4 : 2, 5 : 2, 6 : 2, # average\n",
    "                                                       7 : 3, 8 : 3, 9 : 3, 10 : 3 # good\n",
    "                                                      })\n",
    "all_data[\"SimplPoolQC\"] = all_data.PoolQC.replace({1 : 1, 2 : 1, # average\n",
    "                                             3 : 2, 4 : 2 # good\n",
    "                                            })\n",
    "all_data[\"SimplGarageCond\"] = all_data.GarageCond.replace({1 : 1, # bad\n",
    "                                                     2 : 1, 3 : 1, # average\n",
    "                                                     4 : 2, 5 : 2 # good\n",
    "                                                    })\n",
    "all_data[\"SimplGarageQual\"] = all_data.GarageQual.replace({1 : 1, # bad\n",
    "                                                     2 : 1, 3 : 1, # average\n",
    "                                                     4 : 2, 5 : 2 # good\n",
    "                                                    })\n",
    "all_data[\"SimplFireplaceQu\"] = all_data.FireplaceQu.replace({1 : 1, # bad\n",
    "                                                       2 : 1, 3 : 1, # average\n",
    "                                                       4 : 2, 5 : 2 # good\n",
    "                                                      })\n",
    "all_data[\"SimplFireplaceQu\"] = all_data.FireplaceQu.replace({1 : 1, # bad\n",
    "                                                       2 : 1, 3 : 1, # average\n",
    "                                                       4 : 2, 5 : 2 # good\n",
    "                                                      })\n",
    "all_data[\"SimplFunctional\"] = all_data.Functional.replace({1 : 1, 2 : 1, # bad\n",
    "                                                     3 : 2, 4 : 2, # major\n",
    "                                                     5 : 3, 6 : 3, 7 : 3, # minor\n",
    "                                                     8 : 4 # typical\n",
    "                                                    })\n",
    "all_data[\"SimplKitchenQual\"] = all_data.KitchenQual.replace({1 : 1, # bad\n",
    "                                                       2 : 1, 3 : 1, # average\n",
    "                                                       4 : 2, 5 : 2 # good\n",
    "                                                      })\n",
    "all_data[\"SimplHeatingQC\"] = all_data.HeatingQC.replace({1 : 1, # bad\n",
    "                                                   2 : 1, 3 : 1, # average\n",
    "                                                   4 : 2, 5 : 2 # good\n",
    "                                                  })\n",
    "all_data[\"SimplBsmtFinType1\"] = all_data.BsmtFinType1.replace({1 : 1, # unfinished\n",
    "                                                         2 : 1, 3 : 1, # rec room\n",
    "                                                         4 : 2, 5 : 2, 6 : 2 # living quarters\n",
    "                                                        })\n",
    "all_data[\"SimplBsmtFinType2\"] = all_data.BsmtFinType2.replace({1 : 1, # unfinished\n",
    "                                                         2 : 1, 3 : 1, # rec room\n",
    "                                                         4 : 2, 5 : 2, 6 : 2 # living quarters\n",
    "                                                        })\n",
    "all_data[\"SimplBsmtCond\"] = all_data.BsmtCond.replace({1 : 1, # bad\n",
    "                                                 2 : 1, 3 : 1, # average\n",
    "                                                 4 : 2, 5 : 2 # good\n",
    "                                                })\n",
    "all_data[\"SimplBsmtQual\"] = all_data.BsmtQual.replace({1 : 1, # bad\n",
    "                                                 2 : 1, 3 : 1, # average\n",
    "                                                 4 : 2, 5 : 2 # good\n",
    "                                                })\n",
    "all_data[\"SimplExterCond\"] = all_data.ExterCond.replace({1 : 1, # bad\n",
    "                                                   2 : 1, 3 : 1, # average\n",
    "                                                   4 : 2, 5 : 2 # good\n",
    "                                                  })\n",
    "all_data[\"SimplExterQual\"] = all_data.ExterQual.replace({1 : 1, # bad\n",
    "                                                   2 : 1, 3 : 1, # average\n",
    "                                                   4 : 2, 5 : 2 # good\n",
    "                                                  })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 15))\n",
    "sns.heatmap(all_data.corr(), vmax=1, square=True, cmap=\"RdBu_r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing duplicated variables to avoid multicollinearity\n",
    "all_data.drop(axis=1, columns=[\"1stFlrSF\", \"GarageArea\"], inplace=True) #GarageArea is similar to GarageCars,  1stFlrSF similar to TotalBsmstSF, we keep the two with the highest corr to SalePrice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split into numerical and categorical\n",
    "categorical_features = all_data.select_dtypes(include = [\"object\"]).columns\n",
    "numerical_features = all_data.select_dtypes(exclude = [\"object\"]).columns\n",
    "#numerical_features = numerical_features.drop(\"SalePrice\")\n",
    "all_data_num = all_data[numerical_features]\n",
    "all_data_cat = all_data[categorical_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle remaining missing values for numerical features by using median as replacement\n",
    "print(\"NAs for numerical features in train : \" + str(all_data_num.isnull().values.sum()))\n",
    "all_data_num = all_data_num.fillna(all_data_num.median())\n",
    "print(\"Remaining NAs for numerical features in train : \" + str(all_data_num.isnull().values.sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle remaining missing values for numerical features by using median as replacement\n",
    "all_data_num = all_data_num.fillna(all_data_num.median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skewed_vars = all_data_num.apply(lambda x: skew(x.dropna())) #compute skewness\n",
    "skewed_vars = skewed_vars[skewed_vars > 0.75] #select those with positive skewness (> 0.75)\n",
    "skewed_vars_index = skewed_vars.index\n",
    "all_data_num[skewed_vars_index] = np.log1p(all_data_num[skewed_vars_index]) #apply log transform, note it will include SalePrice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_cat = pd.get_dummies(all_data_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.concat([all_data_num, all_data_cat], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_data.shape)\n",
    "train_X = all_data[:ntrain]\n",
    "test_X = all_data[ntrain:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.linear_model import Ridge, RidgeCV, ElasticNet, ElasticNetCV, LassoCV, LassoLarsCV, Lasso, BayesianRidge, LassoLarsIC\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_folds = 5\n",
    "\n",
    "def rmsle_cv(model):\n",
    "    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(train_X.values)\n",
    "    rmse= np.sqrt(-cross_val_score(model, train_X.values, train_y, scoring=\"neg_mean_squared_error\", cv = kf)).mean()\n",
    "    return(rmse)\n",
    "\n",
    "def predict(model, submission=False):\n",
    "    model.fit(train_X, train_y)\n",
    "    predictions = np.expm1(model.predict(test_X))\n",
    "    print(predictions)\n",
    "    \n",
    "    if submission == True:\n",
    "        my_submission = pd.DataFrame({'Id': test_ID, 'SalePrice': predictions})\n",
    "        my_submission.to_csv('submiqssion_feateng_EDA_nan_log_lasso.csv', index=False)\n",
    "        print(my_submission.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#my_pipeline = Pipeline([(\"imputer\", SimpleImputer(missing_values=\"NaN\", strategy=\"mean\")), (\"lasso\", LassoCV(alphas = [1, 0.1, 0.001, 0.0005]))])\n",
    "lasso = make_pipeline(RobustScaler(), LassoCV(alphas = [1, 0.1, 0.001, 0.0005])) #we use RobustScaler to scale the vars since regression methods are sensitive to outliers\n",
    "rmsle_cv(lasso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#my_pipeline = Pipeline([(\"imputer\", SimpleImputer(missing_values=\"NaN\", strategy=\"mean\")), (\"ElasticNetCV\", ElasticNetCV(alphas=[0.000414], l1_ratio=[.01, .1, .5, .9, .99], max_iter=5000))]) #(\"lasso\", LassoCV(alphas = [1, 0.1, 0.001, 0.0005]))])\n",
    "elasticnet = ElasticNetCV(alphas=[0.000414], l1_ratio=[.01, .1, .5, .9, .99], max_iter=5000)\n",
    "rmsle_cv(elasticnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "krr = KernelRidge()\n",
    "rmsle_cv(krr) #0.12008758541981343"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gboost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05, max_depth=4, max_features='sqrt', min_samples_leaf=15, min_samples_split=10, loss='huber', random_state =5)\n",
    "rmsle_cv(gboost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lightgbm = lgb.LGBMRegressor(objective='regression',num_leaves=5,learning_rate=0.05, n_estimators=720,max_bin = 55, bagging_fraction = 0.8,bagging_freq = 5, feature_fraction = 0.2319,feature_fraction_seed=9, bagging_seed=9,min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)\n",
    "rmsle_cv(lightgbm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find best alpha\n",
    "# alphas2= np.linspace(0.00001,0.01, num=100)\n",
    "\n",
    "# cv_lasso = [-cross_val_score(Lasso(alpha = alpha), train_X, train_y, scoring=\"neg_mean_squared_error\", cv = 5).mean() for alpha in alphas2]\n",
    "# cv_lasso = pd.Series(cv_lasso, index = alphas2)\n",
    "\n",
    "# cv_lasso.plot()\n",
    "# plt.xlabel(\"alpha\")\n",
    "# plt.ylabel(\"rmse\")\n",
    "# print(cv_lasso.sort_values().head()) #will pick 0.000414 as alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with polynomials:\n",
    "#Lasso 0.108392552284 \n",
    "#ElasticNetCV 0.108363828769\n",
    "#######\n",
    "#Lasso 0.11014283035721277 \n",
    "#ElasticNetCV 0.11039019851429285"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AveragingModels(BaseEstimator, RegressorMixin, TransformerMixin):\n",
    "    def __init__(self, models):\n",
    "        self.models = models\n",
    "        \n",
    "    # we define clones of the original models to fit the data in\n",
    "    def fit(self, X, y):\n",
    "        self.models_ = [clone(x) for x in self.models]\n",
    "        \n",
    "        # Train cloned base models\n",
    "        for model in self.models_:\n",
    "            model.fit(X, y)\n",
    "\n",
    "        return self\n",
    "    \n",
    "    #Now we do the predictions for cloned models and average them\n",
    "    def predict(self, X):\n",
    "        predictions = np.column_stack([\n",
    "            model.predict(X) for model in self.models_\n",
    "        ])\n",
    "        return np.mean(predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "averaged_models = AveragingModels(models = (elasticnet, gboost, lasso)) #excluded krr #0.10819004241738783\n",
    "\n",
    "rmsle_cv(averaged_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StackingAveragedModels(BaseEstimator, RegressorMixin, TransformerMixin):\n",
    "    def __init__(self, base_models, meta_model, n_folds=5):\n",
    "        self.base_models = base_models\n",
    "        self.meta_model = meta_model\n",
    "        self.n_folds = n_folds\n",
    "   \n",
    "    # We again fit the data on clones of the original models\n",
    "    def fit(self, X, y):\n",
    "        self.base_models_ = [list() for x in self.base_models]\n",
    "        self.meta_model_ = clone(self.meta_model)\n",
    "        kfold = KFold(n_splits=self.n_folds, shuffle=True, random_state=156)\n",
    "        \n",
    "        # Train cloned base models then create out-of-fold predictions\n",
    "        # that are needed to train the cloned meta-model\n",
    "        out_of_fold_predictions = np.zeros((X.shape[0], len(self.base_models)))\n",
    "        for i, model in enumerate(self.base_models):\n",
    "            for train_index, holdout_index in kfold.split(X, y):\n",
    "                instance = clone(model)\n",
    "                self.base_models_[i].append(instance)\n",
    "                instance.fit(X[train_index], y[train_index])\n",
    "                y_pred = instance.predict(X[holdout_index])\n",
    "                out_of_fold_predictions[holdout_index, i] = y_pred\n",
    "                \n",
    "        # Now train the cloned  meta-model using the out-of-fold predictions as new feature\n",
    "        self.meta_model_.fit(out_of_fold_predictions, y)\n",
    "        return self\n",
    "   \n",
    "    #Do the predictions of all base models on the test data and use the averaged predictions as \n",
    "    #meta-features for the final prediction which is done by the meta-model\n",
    "    def predict(self, X):\n",
    "        meta_features = np.column_stack([\n",
    "            np.column_stack([model.predict(X) for model in base_models]).mean(axis=1)\n",
    "            for base_models in self.base_models_ ])\n",
    "        return self.meta_model_.predict(meta_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_averaged_models = StackingAveragedModels(base_models = (elasticnet, gboost, krr), meta_model = lasso) #0.1083579493404367\n",
    "rmsle_cv(stacked_averaged_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "def rmsle(y, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_averaged_models.fit(train_X.values, train_y)\n",
    "stacked_pred = np.expm1(stacked_averaged_models.predict(test_X.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lightgbm.fit(train_X.values, train_y)\n",
    "lgb_pred = np.expm1(lightgbm.predict(test_X.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost.fit(train_X.values, train_y)\n",
    "xgb_pred = np.expm1(xgboost.predict(test_X.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble = stacked_pred*0.70 + xgb_pred*0.15 + lgb_pred*0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_submission = pd.DataFrame({'Id': test_ID, 'SalePrice': ensemble})\n",
    "my_submission.to_csv('ensembled_model.csv', index=False)\n",
    "print(my_submission.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
