{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources\n",
    "\n",
    "https://www.kaggle.com/sudalairajkumar/simple-exploration-baseline-santander-value - Basic EDA  \n",
    "https://www.kaggle.com/nanomathias/distribution-of-test-vs-training-data - Exploring differences between Test & Train (tSNE)  \n",
    "https://www.kaggle.com/the1owl/love-is-the-answer/notebook  -  Dimensionality reduction & blending  \n",
    "https://www.kaggle.com/ogrellier/santander-46-features/code - Feature transformation  \n",
    "\n",
    "https://lightgbm.readthedocs.io/en/latest/Python-API.html  LightGBM docs  \n",
    "https://xgboost.readthedocs.io/en/latest/python/python_api.html XGBoost docs  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log\n",
    "\n",
    "*June*  \n",
    "> RMSLE: -  \n",
    "Commentary: read through exploratory kernels and Kaggle discussions, performed basic EDA, familiarised myself with data\n",
    "problematics.\n",
    "\n",
    "*02/07/18*  \n",
    "> RMSLE: ```LightGBM = 1.438633```    \n",
    "LB RMSLE: **1.47**  \n",
    "Wall time: 3min 17s  \n",
    "Commentary: reduced features by removing duplicated columns and columns with stdev=0, transformed skewed columns, implemented 5KFold model with LightGBM, hyperparameters from sudalairajkumar's kernel.\n",
    "\n",
    "*03/07/18*  \n",
    "> RMSLE: ```LightGBM = mean: 1.43222, std: 0.02606  ``` & ``` XGBoost = mean: 1.42559, std: 0.02309  ```  \n",
    "LB RMSLE: **1.46**  \n",
    "Wall time: 6min 10s  \n",
    "Commentary: based on the1owl's kernel, reduced features by using the top features from a basic TreeRegressor model, streamlined code, blended LightGBM and XGBoost models\n",
    "\n",
    "*04/07/18*  \n",
    "> RMSLE: ```XGBoost = mean: 1.37952, std: 0.01519  ```  \n",
    "LB RMSLE: **1.44**  \n",
    "Wall time: 1min 16s  \n",
    "Commentary: working on top of previous notebook, top 20 dimensionality reduction components appended, tuned hyperparams, dropped LightGBM\n",
    "\n",
    "*15/07/18*\n",
    "> LightGBM ```RMSLE mean: 1.34165, std: 0.02615```  \n",
    "LB RMSLE: **1.40**   \n",
    "Wall time: 3min 13s  \n",
    "Commentary: removed dimensionality reduction agg features, added new statistical agg features, switch from 400 to 1200 Random forest most important features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "from datetime import timedelta\n",
    "from contextlib import contextmanager\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from lightgbm import LGBMRegressor, Dataset\n",
    "from xgboost import XGBRegressor, DMatrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.decomposition import PCA, TruncatedSVD, FastICA\n",
    "from sklearn.random_projection import GaussianRandomProjection, SparseRandomProjection\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "\n",
    "def rmsle(y, predictions):\n",
    "    return np.sqrt(np.mean(np.power(np.log1p(y) - np.log1p(predictions), 2)))\n",
    "\n",
    "@contextmanager\n",
    "def timer(title=\"\"):\n",
    "    start = time()\n",
    "    yield\n",
    "    print(\"{} done in {:.0f}s\".format(title, str(timedelta(seconds=time()-start))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 4459 rows, 4993 columns \n",
      "Test: 49342 rows, 4992 columns\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(\"C:\\\\Users\\\\heret\\\\Downloads\\\\Santander\\\\train.csv\")\n",
    "test = pd.read_csv(\"C:\\\\Users\\\\heret\\\\Downloads\\\\Santander\\\\test.csv\")\n",
    "\n",
    "print((\"Train: {} rows, {} columns \\nTest: {} rows, {} columns\".format(train.shape[0], train.shape[1], test.shape[0], test.shape[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feat_selection(X, n_feats=1200):\n",
    "    col = [c for c in X.columns if c not in ['ID', 'target']]\n",
    "\n",
    "    scl = StandardScaler()\n",
    "    x1, x2, y1, y2 = train_test_split(X[col], X[\"target\"].values, test_size=0.20, random_state=5)\n",
    "    model = RandomForestRegressor(n_jobs = -1, random_state = 7)\n",
    "    model.fit(scl.fit_transform(x1), y1)\n",
    "    print(rmsle(y2, model.predict(scl.transform(x2))))\n",
    "\n",
    "    col = pd.DataFrame({'importance': model.feature_importances_, 'feature': col}).sort_values(by=['importance'], ascending=[False])[:n_feats]['feature'].values\n",
    "    print(\"Selected {} most important features\".format(col.size))\n",
    "    importances = model.feature_importances_\n",
    "    # indices = np.argsort(importances)\n",
    "    # plt.figure(1)\n",
    "    # plt.title('Feature Importances')\n",
    "    # plt.barh(range(len(indices)), importances[indices], color='r', align='center')\n",
    "    # plt.xlabel('Relative Importance')\n",
    "    # plt.xlim(0,0.012)\n",
    "    return col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7700695707637661\n",
      "Selected 1200 most important features\n",
      "train_X: (4459, 1200), test_X: (49342, 1200), train_y: (4459,)\n"
     ]
    }
   ],
   "source": [
    "col = feat_selection(train)\n",
    "ids = test[\"ID\"]\n",
    "train_y = np.log1p(train[\"target\"])\n",
    "train_X, test_X = train[col], test[col]\n",
    "\n",
    "print(\"train_X: {}, test_X: {}, train_y: {}\".format(train_X.shape, test_X.shape, train_y.shape))\n",
    "del train, test, col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replacing 0 for NaNs\n",
    "#creating aggregated variables with the information for each observaton\n",
    "for dataset in [train_X, test_X]:\n",
    "    dataset.replace(0, np.nan, inplace=True)\n",
    "\n",
    "    dataset['nans'] = dataset.isnull().sum(axis=1)\n",
    "    dataset['median'] = dataset.median(axis=1)\n",
    "    dataset['mean'] = dataset.mean(axis=1)\n",
    "    dataset['sum'] = dataset.sum(axis=1)\n",
    "    dataset['std'] = dataset.std(axis=1)\n",
    "    dataset['kur'] = dataset.kurtosis(axis=1)\n",
    "    dataset['max'] = dataset.max(axis=1)\n",
    "    dataset['min'] = dataset.min(axis=1)\n",
    "    dataset['skew'] = dataset.skew(axis=1)\n",
    "    dataset['sum'] = dataset.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def fold_boost(X,y,T):\n",
    "    \n",
    "    folds = KFold(n_splits=5, shuffle=True, random_state=23)\n",
    "    folds_scores_lgbm=[]\n",
    "    folds_scores_xgb=[]\n",
    "    valid_preds_lgbm = np.zeros(X.shape[0])\n",
    "    valid_preds_xgb = np.zeros(X.shape[0])\n",
    "    test_preds_lgbm = np.zeros(T.shape[0])\n",
    "    test_preds_xgb = np.zeros(T.shape[0])\n",
    "\n",
    "    for fold_no, (train_idx, valid_idx) in enumerate(folds.split(X,y)):\n",
    "\n",
    "        X_train = X.iloc[train_idx]\n",
    "        y_train = y.iloc[train_idx]\n",
    "        X_valid = X.iloc[valid_idx]\n",
    "        y_valid = y.iloc[valid_idx]        \n",
    "        print(\"X_train: {} y_train: {} X_valid: {} y_valid: {}\".format(X_train.shape, y_train.shape, X_valid.shape, y_valid.shape))\n",
    "\n",
    "        #LightGBM\n",
    "        params = {\"objective\":'regression',\"num_leaves\":144, \"learning_rate\":0.005, \"max_depth\":13, \"metric\":'rmse',\"is_training_metric\":True, \"max_bin\" : 55, \"bagging_fraction\" : 0.8, \"bagging_freq\" : 5, \"feature_fraction\" : 0.9}\n",
    "        model = lgb.train(params=params, train_set=Dataset(X_train, label=y_train),\n",
    "                          num_boost_round=10000, valid_sets=Dataset(X_valid, label=y_valid),\n",
    "                          verbose_eval=False, early_stopping_rounds=100)\n",
    "        test_preds_lgbm += np.expm1(model.predict(T, num_iteration=model.best_iteration)) / folds.n_splits\n",
    "        valid_preds_lgbm[valid_idx] = model.predict(X_valid, num_iteration=model.best_iteration)\n",
    "        score = rmsle(np.expm1(y_valid), np.expm1(valid_preds_lgbm[valid_idx]))\n",
    "        folds_scores_lgbm.append(score)\n",
    "        print(\"\\nFold %2d RMSLE : %.6f\\n\" % (fold_no + 1, score))\n",
    "\n",
    "        #XGB\n",
    "#         watchlist = [(DMatrix(X_train, y_train), 'train'), (DMatrix(X_valid, y_valid), 'valid')]\n",
    "#         params = {'objective': 'reg:linear', 'booster': 'gbtree', \"learning_rate\":0.01, \"max_depth\":30, \"min_child_weight\":30, \"gamma\":0, \"subsample\": 0.75, \"colsample_bytree\": 0.05,\"colsample_bylevel\":0.7, \"n_jobs\": -1, \"reg_lambda\": 0.1}\n",
    "#         model = xgb.train(params, DMatrix(X_train, y_train), 5000,  watchlist, maximize=False, verbose_eval=False, early_stopping_rounds=100)\n",
    "#         test_preds_xgb += np.expm1(model.predict(DMatrix(T), ntree_limit=model.best_ntree_limit)) / folds.n_splits\n",
    "#         valid_preds_xgb[valid_idx] = model.predict(DMatrix(X_valid), ntree_limit=model.best_ntree_limit)\n",
    "#         score = rmsle(np.expm1(y_valid), np.expm1(valid_preds_xgb[valid_idx]))\n",
    "#         folds_scores_xgb.append(score)\n",
    "#         print(\"\\nFold %2d RMSLE : %.6f\\n\" % (fold_no + 1, score))\n",
    "    \n",
    "    print(\"LightGBM RMSLE mean: {}, std: {}\".format(np.mean(folds_scores_lgbm).round(5), np.std(folds_scores_lgbm).round(5)))\n",
    "    print(\"XGBoost RMSLE mean: {}, std: {}\".format(np.mean(folds_scores_xgb).round(5), np.std(folds_scores_xgb).round(5)))\n",
    "    return (valid_preds_lgbm, test_preds_lgbm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (3567, 1209) y_train: (3567,) X_valid: (892, 1209) y_valid: (892,)\n",
      "\n",
      "Fold  1 RMSLE : 1.316284\n",
      "\n",
      "X_train: (3567, 1209) y_train: (3567,) X_valid: (892, 1209) y_valid: (892,)\n",
      "\n",
      "Fold  2 RMSLE : 1.349357\n",
      "\n",
      "X_train: (3567, 1209) y_train: (3567,) X_valid: (892, 1209) y_valid: (892,)\n",
      "\n",
      "Fold  3 RMSLE : 1.379036\n",
      "\n",
      "X_train: (3567, 1209) y_train: (3567,) X_valid: (892, 1209) y_valid: (892,)\n",
      "\n",
      "Fold  4 RMSLE : 1.355482\n",
      "\n",
      "X_train: (3568, 1209) y_train: (3568,) X_valid: (891, 1209) y_valid: (891,)\n",
      "\n",
      "Fold  5 RMSLE : 1.308081\n",
      "\n",
      "LightGBM RMSLE mean: 1.34165, std: 0.02615\n",
      "XGBoost RMSLE mean: nan, std: nan\n",
      "Wall time: 3min 13s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\heret\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:2957: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\heret\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "C:\\Users\\heret\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:135: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  keepdims=keepdims)\n",
      "C:\\Users\\heret\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:105: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean, rcount, out=arrmean, casting='unsafe', subok=False)\n",
      "C:\\Users\\heret\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:127: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "valid_preds_lgbm, test_preds_lgbm = fold_boost(train_X, train_y, test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000137c73</td>\n",
       "      <td>7.636659e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00021489f</td>\n",
       "      <td>1.773018e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0004d7953</td>\n",
       "      <td>2.909381e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00056a333</td>\n",
       "      <td>5.305285e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00056d8eb</td>\n",
       "      <td>1.193637e+06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID        target\n",
       "0  000137c73  7.636659e+06\n",
       "1  00021489f  1.773018e+06\n",
       "2  0004d7953  2.909381e+06\n",
       "3  00056a333  5.305285e+06\n",
       "4  00056d8eb  1.193637e+06"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = test_preds_lgbm\n",
    "submissions = pd.DataFrame({\"ID\":ids, \"target\":predictions})\n",
    "submissions.to_csv(\"santanderv4.csv\", index=False)\n",
    "submissions.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
